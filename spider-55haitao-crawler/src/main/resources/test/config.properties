## 执行抓取工作的线程数量
crawler.property.worker.count=1

## 当工作线程获取不到待抓取的urls时，线程休眠时间，单位：毫秒
crawler.property.sleepTimeNoneUrls=5000

## 每次从Controller层批量拉取url数量，然后填充到本地缓存队列中
crawler.property.documentInputCount=100
## 工作线程每次从本地缓存队列中批量获取url数量
crawler.property.documentOutputCount=20
## Crawler端从Controller端定时拉取任务配置的时间间隔
crawler.property.taskReloadFetchInterval=10000
## Crawler端向Controller端定时发送心跳信息的时间间隔
crawler.property.heartbeatInterval=6000
## Crawler端从Controller端定时拉取代理IPs的时间间隔
crawler.property.proxyFetchInterval=60000
## 最近连续多少次抓取失败则删除Url
crawler.property.deleteThresholdOnCrawledFailedTimes=5

## Thrift连接时的属性配置
crawler.property.thrift.serviceIP=172.16.7.161
crawler.property.thrift.servicePort=7911
crawler.property.thrift.connectTimeOut=30000
crawler.property.thrift.initialBufferCapacity=10240
crawler.property.thrift.maxLength=1024000

crawler.property.tempoInterval=10000


##upyun 服务  操作员信息
upyun.bucketName=haitao55-test
upyun.userName=zhaoxinluo
upyun.password=zhaoxinluo
upyun.address=http://haitao55-test.b0.upaiyun.com


##kafka
kafka.haitao.topic=spider_crawler_55haitao
kafka.haitao.key=www.6pm.com
#kafka producer config
kafka.bootstrap.servers=172.16.7.161:9092
kafka.acks=all
kafka.retries=3
kafka.batch.size=16384
kafka.linger.ms=1
kafka.buffer.memory=33554432
kafka.key.serializer=org.apache.kafka.common.serialization.StringSerializer
kafka.value.serializer=org.apache.kafka.common.serialization.StringSerializer
##kafka consumer config

## 返利网合作支持功能中的邮件相关配置
fanli.support.email.send.interval=60000
fanli.support.email.sender.address=liushizhen@55haitao.com
fanli.support.email.sender.password=xxx
fanli.support.email.receiver.addresses=liushizhen@55haitao.com,liusz_ok@126.com