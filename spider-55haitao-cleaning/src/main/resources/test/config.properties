mongo.host=127.0.0.1
mongo.port=27017
mongo.dbname=spider_55haitao
mongo.user=
mongo.passwd=

## 系统待处理数据所在位置的根路径
cleaning.data.input.root=/home/arthur/experiments/spider-cleaning/inputdatas
##后处理items数据存放的根路径
cleaning.data.output.root.path=/home/denghuan/data
##后处理item数据存放的文件名称
cleaning.data.output.file=55haitao_cleaning_items.json
##后处理清洗后完整items数据存放的根路径
cleaning.afte.data.output.root.path=/data/cleaning_after_datas
## 监视线程执行周期,单位:毫秒
cleaning.data.input.watch.interval=600000

## 数据处理线程池属性:最小线程数
cleaning.siteDir.threadPoolProps.corePoolSize=5
## 数据处理线程池属性:最大线程数
cleaning.siteDir.threadPoolProps.maximumPoolSize=50
## 数据处理线程池属性:多余线程最大存活时间
cleaning.siteDir.threadPoolProps.keepAliveTime=10000
## 数据处理线程池属性:任务队列长度
cleaning.siteDir.threadPoolProps.workQueueSize=1000

##kafka-from-crawler consumer conifg
kafka.from.crawler.auto.offset.reset=earliest
kafka.from.crawler.servers=127.0.0.1:9092
kafka.from.crawler.topic=spider_crawler_55haitao_test
kafka.from.crawler.auto.commit.interval.ms=1000
kafka.from.crawler.session.timeout.ms=30000
kafka.from.crawler.enable.auto.commit=true
kafka.from.crawler.max.poll.records=1
kafka.from.crawler.group.id=spider_item_group_test
kafka.from.crawler.key.serializer=org.apache.kafka.common.serialization.StringDeserializer
kafka.from.crawler.value.serializer=org.apache.kafka.common.serialization.StringDeserializer

#kafka-to-SE producer config
kafka.to.se.servers=127.0.0.1:9092
kafka.to.se.topic=search_item_topic_test
kafka.to.se.acks=all
kafka.to.se.retries=3
kafka.to.se.batch.size=16384
kafka.to.se.linger.ms=1
kafka.to.se.buffer.memory=33554432
kafka.to.se.key.serializer=org.apache.kafka.common.serialization.StringSerializer
kafka.to.se.value.serializer=org.apache.kafka.common.serialization.StringSerializer